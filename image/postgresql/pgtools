#!/usr/bin/env bash
#set -Eeo pipefail

# global variable
do_config=0
drop_auto_failover=0
actual_drop_auto_failover=0
do_hba=0
do_pause=0
do_query=0
query_database=postgres
timeout=180
querys=()
set_auto_failover_config=0
set_auto_failover=0
do_restart_postgresql=0
do_restart_auto_failover=0
PG_CONFIG=PG_CONFIG_
PG_HBA=PG_HBA_
no_error=0
ready_accept=0
SUCCESS="exec_success"
switchover=0
backup=0
backup_info=0
restore=0
backup_delete=0

# set env
DATA=${DATA:-/var/lib/postgresql/data}
PGDATA=${DATA}/pg_data
BARMAN_DATA=${BARMAN_DATA:-/var/lib/postgresql/data/barman/data}
BARMAN_CONF=${BARMAN_CONF:-/var/lib/postgresql/data/barman/config}
BARMAN_BACKUPNAME=${BARMAN_BACKUPNAME:-postgresql-backup}
export PGDATA
#export XDG_CONFIG_HOME=${DATA}/auto_failover
#export XDG_DATA_HOME=${DATA}/auto_failover
if [ "$PG_MODE" = monitor -o "$PG_MODE" = readwrite -o "$PG_MODE" = readonly ]; then
	run_port=$(cat ${XDG_CONFIG_HOME}/pg_autoctl/${PGDATA}/pg_autoctl.cfg | grep -w port | cut -d "=" -f 2 | tr -d " ")
else
	run_port=$(cat ${PGDATA}/postgresql.conf | grep -w port | grep '[0-9]' | tail -n 1 | cut -d '=' -f 2 | cut -d "#" -f 1 | tr -d " ")
fi

# run as postgres
if [ "$(id -u)" = '0' ]; then
	#exec su-exec postgres "$BASH_SOURCE" "$@" alpine
	#exec gosu postgres "$BASH_SOURCE" "$@" alpine
	exec gosu postgres "$BASH_SOURCE" "$@"
fi

############ barman config start
# local
local_pg="[$BARMAN_BACKUPNAME]
description = 'Local PostgreSQL'
backup_method = local-rsync
backup_options = concurrent_backup
archiver = on
conninfo = host=127.0.0.1 port=5432 user=postgres dbname=postgres
parallel_jobs = 1
reuse_backup = link
path_prefix = '/var/lib/postgresql/bin'
;; retention_policy = REDUNDANCY 10
wal_retention_policy = main"

# streaming
streaming_pg=""
############ barman config end

# parse argument
while getopts "acdDe:hHp:q:w:Q:norRs:S:bE:vB" arg
do
	case $arg in
		a)
			ready_accept=1
			;;
		c)
			do_config=1
			;;
		n)
			no_error=1
			;;
		o)
			switchover=1
			;;
		d)
			drop_auto_failover=1
			;;
		D)
			actual_drop_auto_failover=1
			;;
		e)
			env_name=${OPTARG%%=*}
			env_value=${OPTARG#*=}
			export "$env_name"="$env_value"
			;;
		h)
			echo "pgtools is util for manage postgresql"
			echo "every command in container run by it"
			echo "  -a        init finish and connect success."
			echo "  -c        flush postgresql config file"
			echo "  -d        drop the auto_failover node"
			echo "  -D        actual drop the auto_failover node, this node can't start again"
			echo "  -e env    set environment befor running"
			echo "  -h        help"
			echo "  -H        flush hba file"
			echo "  -p action pause/resume  pause or resume start postgresql"
			echo "  -q query  run the query"
			echo "  -w second timeout for query"
			echo "  -Q        run the query on this database"
			echo "  -n        if failed not return error"
			echo "  -o        perform switchover"
			echo "  -r        restart postgresql"
			echo "  -R        stop auto_failover"
			echo "  -E        restore cluster from s3 backup"
			echo "  -v        get s3 backup information in json format"
			echo "  -b        backup cluster to s3"
			echo "  -B        backup_delete by policy"
			echo "  -s value  set auto_failover config: pg_autoctl config set.."
			echo "  -S value  set auto_failover: pg_autoctl set.."
			exit 0
			;;
		H)
			do_hba=1
			;;
		p)
			do_pause="$OPTARG"
			;;
		q)
			do_query=1
			querys[${#querys[@]}]="$OPTARG"
			;;
		w)
			timeout="$OPTARG"
			;;
		Q)
			query_database="$OPTARG"
			;;
		r)
			do_restart_postgresql=1
			;;
		R)
			do_restart_auto_failover=1
			;;
		E)
			BACKUP_ID=${OPTARG%%=*}
			RESTORE_TIME=${OPTARG#*=}
			if [ "$BACKUP_ID" == "$RESTORE_TIME" ]; then
				RESTORE_TIME=""
			fi
			restore=1
			;;
		v)
			backup_info=1
			;;
		s)
			set_auto_failover_config="$OPTARG"
			;;
		S)
			set_auto_failover="$OPTARG"
			;;
		b)
			backup=1
			;;
		B)
			backup_delete=1
			;;
		?)
			echo "unknow argument"
			exit 1
			;;
	esac
done


function check_error() {
	ret_code="$1"
	msg="$2"
	if [ "$ret_code" != 0 -a "$no_error" = 0 ]; then
		echo `date`": $msg"
		exit 1
	fi
}

function reload_postgresql(){
	pg_ctl reload -D "$PGDATA"
	check_error "$?" "reload postgresql failed"
}

function restart_postgresql(){
	if [ "$PG_MODE" = monitor -o "$PG_MODE" = readwrite -o "$PG_MODE" = readonly ]; then
		# auto_failover start it
		pg_ctl restart -D $PGDATA
	else
		pg_ctl restart -D $PGDATA
	fi
	check_error $? "restart postgresql failed"
}

function stop_auto_failover(){
	# auto_failover can't restart. so stop it. k8s or docker will start it
	touch ${ASSIST}/stop
	for i in `seq 1 100`
	do
		pg_autoctl stop --pgdata=$PGDATA
		if [ $? == 0 ]; then
			exit 0
		fi
		sleep 6
	done

	pg_autoctl stop --pgdata=$PGDATA
	check_error $? "stop auto_failover failed"
}

function pg_is_running() {
	ret=1
	output=$(pgtools -w 1 -q "select 1;")
	if [ "$output" == 1 -a "$?" == 0 ]; then
		ret=0
	fi
	echo $ret
}

function barman_streaming_config() {
	file=$1
	if [ ! -e $file ]; then
		echo "$streaming_pg" > "$file"
	fi
}

function barman_rsync_config() {
	file=$1
	if [ ! -e $file ]; then
		echo "$local_pg" > "$file"
	fi
}

function generate_barman_config() {
	backup_method=$1
	file=$2
	if [ "$backup_method" == "streaming" ]; then
		barman_streaming_config "$file"
	elif [ "$backup_method" == "rsync" ]; then
		barman_rsync_config "$file"
	else
		check_error 1 "unsupported backup method"
	fi
}

function generate_s3_profile() {
	AWS_ACCESS_KEY_ID=$1
	AWS_SECRET_ACCESS_KEY=$2
	echo -e "$AWS_ACCESS_KEY_ID\n$AWS_SECRET_ACCESS_KEY\nNone\njson\n" | aws configure --profile default > /dev/null
}

function check_s3_env() {
	if [ ! $S3_ENDPOINT ] || [ ! $S3_BUCKET ]; then
		check_error 1 "S3_ENDPOINT or S3_BUCKET is not set"
	fi
	if [ ! $S3_ACCESS_KEY ] || [ ! $S3_SECRET_KEY ]; then
		check_error 1 "S3_ACCESS_KEY or S3_SECRET_KEY is not set"
	fi
}

function check_s3_valid() {
	S3_ENDPOINT=$1
	S3_BUCKET=$2
	S3_PATH=$3
	barman-cloud-backup --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME -t
	check_error $? "s3 connect failed"
}

function check_and_set_backup_param() {
	S3_ENDPOINT=$1
	S3_BUCKET=$2
	S3_PATH=$3
	need_reload=0
	# if wal archive is not enable, reset archive_command and archive_timeout param, return
	if [ "$archive" != "on" ]; then
		echo "archive_command = '/bin/true'" >> $PGDATA/postgresql.conf
		echo "archive_timeout = 0" >> $PGDATA/postgresql.conf
		reload_postgresql
		return 0
	fi
	archive_command="barman-cloud-wal-archive --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME %p"
	archive_command=$(add_s3_param "$archive_command")

	output=$(pgtools -w 1 -q "select setting from pg_settings where name = 'wal_level' and setting = 'minimal';")
	if [ $output ]; then
		check_error 1 "wal_level is minimal, please set wal_level to replica or logical"
	fi
	output=$(pgtools -w 1 -q "select setting from pg_settings where name = 'archive_mode' and setting = 'off';")
	if [ $output ]; then
		check_error 1 "archive_mode is off, please turn on archive_mode"
	fi
	output=$(pgtools -w 1 -q "select setting from pg_settings where name = 'archive_command';")
	if [[ $output != $archive_command ]]; then
		need_reload=1
		echo "archive_command = '$archive_command'" >> $PGDATA/postgresql.conf
	fi
	output=$(pgtools -w 1 -q "select setting from pg_settings where name = 'archive_timeout';")
	if [ $output == 0 ]; then
		need_reload=1
		echo "archive_timeout = 60" >> $PGDATA/postgresql.conf
	fi
	if [ $need_reload == 1 ]; then
		reload_postgresql
		pgtools -q "select pg_switch_wal();"
	fi
}

function add_s3_param() {
	cmd=$1

	# compression
	if [ "$compression" == "gzip" ]; then
		cmd="$cmd --gzip"
	elif [ "$compression" == "bzip2" ]; then
		cmd="$cmd --bzip2"
	elif [ "$compression" == "snappy" ]; then
		cmd="$cmd --snappy"
	fi

	# encryption
	if [ "$encryption" != "none" ]; then
		cmd="$cmd -e $encryption"
	fi

	echo $cmd
}

function s3_backup() {
	role=$(pgtools -w 1 -q "show transaction_read_only")
	if [ $role == "off" ]; then
		role="master"
	elif [ $role == "on" ]; then
		role="slave"
	fi
	backup_cmd="barman-cloud-backup --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME"
	backup_cmd=$(add_s3_param "$backup_cmd")
	check_s3_env
	generate_barman_config rsync $BARMAN_CONF/$BARMAN_BACKUPNAME.conf
	generate_s3_profile $S3_ACCESS_KEY $S3_SECRET_KEY
	check_s3_valid $S3_ENDPOINT $S3_BUCKET $S3_PATH
	check_and_set_backup_param $S3_ENDPOINT $S3_BUCKET $S3_PATH
	if [ $role == "master" ]; then
		$backup_cmd
		if [ $? != 0 ]; then
			check_error 1 "backup failed, please check error log"
		fi
	fi
}

function get_backup_info_format_json() {
	check_s3_env
	generate_s3_profile $S3_ACCESS_KEY $S3_SECRET_KEY
	check_s3_valid $S3_ENDPOINT $S3_BUCKET $S3_PATH
	barman-cloud-backup-list --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME --format json
}

function delete_backup_by_id() {
	backupid=$1
	barman-cloud-backup-delete --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME --backup-id $backupid
}

function delete_backup_by_policy() {
	barman-cloud-backup-delete --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME --retention-policy "$BACKUP_POLICY"
}

function delete_backup() {
	check_s3_env
	generate_s3_profile $S3_ACCESS_KEY $S3_SECRET_KEY
	check_s3_valid $S3_ENDPOINT $S3_BUCKET $S3_PATH
	if [ ! "$BACKUP_POLICY" ]; then
		check_error 1 "delete_backup but BACKUP_POLICY env is not set."
	fi
	delete_backup_by_policy
	if [ $? != 0 ]; then
		check_error 1 "delete_backup failed, please check error log"
	fi
}

function check_backup_exists() {
	BACKUP_ID=$1
	ret=1
	output=$(barman-cloud-backup-list --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME | awk -v target=$BACKUP_ID '{ if (NR>1 && $1==target) {print $1}}')
	if [ "$output" -a "$?" == 0 ]; then
		ret=0
	fi
	echo $ret
}

function check_time_valid() {
	BACKUP_ID=$1
	recovery_time=$2

	target_timestamp=$(date --date "$recovery_time" '+%s')
	backup_end_timestamp=$(barman-cloud-backup-list --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME | awk  -F '     ' -v target=$BACKUP_ID '{ if (NR>1 && $1==target) {cmd="date --date \""$2"\" +%s";system(cmd)}}')
	if [ $target_timestamp -lt $backup_end_timestamp ]; then
		check_error 1 "target time is not valid. backup_end_time is $(date -d +@"$backup_end_timestamp"), target_time is $(date -d +@"$target_timestamp")"
	fi
}

function s3_restore_by_time() {
	RESTORE_TIME=$1
	touch $PGDATA/recovery.signal
	echo "restore_command = 'barman-cloud-wal-restore --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME %f %p'" >> $PGDATA/postgresql.conf
	echo "recovery_target_timeline = 'latest'" >> $PGDATA/postgresql.conf
	echo "recovery_target_time = '$RESTORE_TIME'" >> $PGDATA/postgresql.conf
	echo "archive_command = '/bin/true'" >> $PGDATA/postgresql.conf
	echo "archive_timeout = 0" >> $PGDATA/postgresql.conf
}

# need stop postgresql service
# call:
#     pgtools -E "backupid"="restore_time"
#     eg: pgtools -E "20220926T084704"="2022-09-26 16:42:50"
function s3_restore() {

	ret=$(pg_is_running)
	if [ $ret == 0 ]; then
		check_error 1 "restore failed, postgresql service must be shut down"
	fi

	BACKUP_ID=$1
	param=$2          # recovery_target_time
	if [ ! "$BACKUP_ID" ] || [ ! "$param" ]; then
		check_error 1 "BACKUP_ID or recovery_target_time is not set"
	fi

	check_s3_env
	generate_barman_config rsync $BARMAN_CONF/$BARMAN_BACKUPNAME.conf
	generate_s3_profile $S3_ACCESS_KEY $S3_SECRET_KEY
	check_s3_valid $S3_ENDPOINT $S3_BUCKET $S3_PATH
	ret=$(check_backup_exists $BACKUP_ID)
	if [ "$ret" != 0 -o "$?" != 0 ]; then
		check_error 1 "BACKUP $BACKUP_ID does not exist, please select another backup id"
	fi
	check_time_valid "$BACKUP_ID" "$param"

	rm -rf $PGDATA
	barman-cloud-restore --cloud-provider aws-s3 -P default --endpoint-url $S3_ENDPOINT s3://$S3_BUCKET/$S3_PATH $BARMAN_BACKUPNAME $BACKUP_ID $PGDATA
	if [ $? != 0 ]; then
		check_error 1 "restore failed, please check error log"
	fi
	s3_restore_by_time "$param"
}

if [ "${do_config}" = 1 ]; then

	env | grep "^$PG_CONFIG" | while read item
	do
		conf=${item#*${PG_CONFIG}}
		#conf_name=${conf%%=*}
		#conf_value=${conf#*=}
		#name_context=$(eval echo '$'$conf_name)
		echo "$conf" >> $PGDATA/postgresql.conf
	done

	reload_postgresql
	echo $SUCCESS
fi


if [ "${do_hba}" = 1 ]; then
	hba_file=$PGDATA/pg_hba.conf
	hba_file_origin=$PGDATA/pg_hba.conf.origin

	if [ ! -e "$hba_file_origin" ]; then
		cp "$hba_file" "$hba_file_origin"
	fi

	cp "$hba_file_origin" "$hba_file"
	env | grep "^$PG_HBA" | while read item
	do
		conf=${item#*${PG_HBA}}
		#conf_name=${conf%%=*}
		conf_value=${conf#*=}
		echo "$conf_value" >> "$hba_file"
	done
	echo "host all all all md5" >> "$hba_file"
	echo "host replication all all md5" >> "$hba_file"

	reload_postgresql
	echo $SUCCESS
fi

if [ "$ready_accept" = 1 ]; then
	init_file=init_finish

	pg_isready -p $run_port 1>/dev/null 2>&1
	if [ $? != 0 ]; then
		check_error 1 "can't connect database."
	fi

	cat "${ASSIST}/${init_file}"
	if [ $? != 0 ]; then
		check_error 1 "init not finish"
	fi
fi

if [ "$switchover" = 1 ]; then
	for i in `seq 1 100`
	do
		pg_autoctl perform switchover --pgdata $PGDATA --formation primary
		if [ $? == 0 ]; then
			exit 0
		fi
		sleep 6
	done
	pg_autoctl perform switchover --pgdata $PGDATA --formation primary
	check_error 1 "switchover failed"
fi

if [ "${do_query}" = 1 ]; then
	try_num=$timeout
	for i in `seq 1 $try_num`
	do
		pg_isready -p $run_port 1>/dev/null 2>&1
		if [ $? != 0 ]; then
			echo "database is not ready"
			if [ $i = "$try_num" ]; then
				check_error 1 "failed."
			fi
			sleep 1
		else
			break
		fi
	done

	# timeout 0
	pg_isready -p $run_port 1>/dev/null 2>&1
	if [ $? != 0 ]; then
		check_error 1 "failed."
	fi
		
	max_querys=`expr ${#querys[@]} - 1`
	for i in `seq 0 "$max_querys"`
	do
		psql -p $run_port -t -A --dbname $query_database -U postgres -c "${querys[$i]}"
		check_error $? "run query ${querys[$i]} failed"
	done
fi

if [ "${do_restart_postgresql}" = 1 ]; then
	restart_postgresql
	echo $SUCCESS
fi

if [ "${do_restart_auto_failover}" = 1 ]; then
	# auto_failover can't restart. so stop it. k8s or docker will start it
	stop_auto_failover
fi

if [ "${set_auto_failover_config}" != 0 ]; then
	pg_autoctl config set --pgdata $PGDATA $set_auto_failover_config
	check_error $? "set auto_failover_config failed"
	echo $SUCCESS
fi

if [ "${set_auto_failover}" != 0 ]; then
	pg_autoctl set $set_auto_failover  --pgdata $PGDATA
	check_error $? "set auto_failover failed"
	echo $SUCCESS
fi

if [ "$do_pause" != 0 ]; then
	if [ "$do_pause" = pause ]; then
		touch "${ASSIST}/pause"
		sleep 1 #waiting flush to disk
	fi
	if [ "$do_pause" = resume ]; then
		rm -rf "${ASSIST}/pause"
	fi
fi

if [ "${drop_auto_failover}" = 1 ]; then
	for i in `seq 1 100`
	do
		pg_autoctl drop node --pgdata=$PGDATA --force
		if [ $? == 0 ]; then
			exit 0
		fi
		sleep 6
	done
	pg_autoctl drop node --pgdata=$PGDATA --force
	check_error $? "drop auto_failover failed"
fi

if [ "${actual_drop_auto_failover}" = 1 ]; then
	touch ${ASSIST}/actual_drop
	for i in `seq 1 100`
	do
		pg_autoctl drop node --pgdata=$PGDATA --force
		if [ $? == 0 ]; then
			exit 0
		fi
		sleep 6
	done
	pg_autoctl drop node --pgdata=$PGDATA --force
	check_error $? "drop auto_failover failed"
fi

if [ "${backup}" = 1 ]; then
	s3_backup
	echo $SUCCESS
fi

if [ "${restore}" = 1 ]; then
	s3_restore "$BACKUP_ID" "$RESTORE_TIME"
	echo $SUCCESS
fi

if [ "${backup_info}" = 1 ]; then
	get_backup_info_format_json
fi

if [ "${backup_delete}" = 1 ]; then
	delete_backup_by_policy
fi

exit 0
